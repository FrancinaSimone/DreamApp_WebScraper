### DreamApp_WebScrapper: The Swiss Army Wand of Web Scraping Tools for DreamApp_Core 🌐✨

Welcome to `DreamApp_WebScrapper`, a public submodule designed to seamlessly integrate with our private core repository, `DreamApp_Core`. This is your one-stop shop for all things web scraping that fuel the magical engine behind DreamApp.

#### What's Inside the Box 📦
- **Versatile Web Scrapers**: From gathering folklore stories to scraping the latest memes, we have a scraper for that.
- **Optimized for DreamApp_Core**: Designed with love to play nice with our core repository. Like peanut butter and jelly, baby.
- **Open for Extensions**: Got a new data source? Easily extend this submodule with your own web scraper.

#### Why Public? 🌍
Great tools should be shared. While `DreamApp_Core` remains our secret sauce, we believe in contributing to the community by making all our scraping utilities publicly available. Feel free to fork, contribute, or take inspiration.

---

# DreamApp_WebScrapper 🌐✨

Welcome to `DreamApp_WebScrapper`, the public submodule designed to seamlessly integrate with our private `DreamApp_Core`. This is your one-stop shop for all web scraping utilities that fuel the magic behind DreamApp.

## Table of Contents
- [Getting Started](#getting-started)
- [Dependencies](#dependencies)
- [Usage](#usage)
- [Contributing](#contributing)

## Getting Started 🚀

Clone this submodule into your `DreamApp_Core` repository or any other project where you need web scraping utilities.

```bash
git clone https://github.com/FrancinaSimone/DreamApp_WebScrapper.git
```

## Dependencies 📚

- Python 3.x
- BeautifulSoup4
- Requests

Install the dependencies using pip:

```bash
pip install beautifulsoup4 requests
```

## Usage 🛠

### Story Scraper Example

Here's how to use the Story Scraper for `FirstPeople.us`:

1. **Navigate to the Web Scraper Directory**
    ```bash
    cd path/to/DreamApp_WebScrapper/src/core/
    ```

2. **Run the Script**
    ```bash
    python advanced_webscraper.py
    ```

The scraped stories will be saved in CSV files under the directory specified in the script.

## Contributing 🤝

Feel free to fork, make changes, and create pull requests. All contributions are welcome!