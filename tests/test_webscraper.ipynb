{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Webscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Book of Web Scrapping: A Guide to Collecting Stories from FirstPeople.us\n",
    "\n",
    "# The Magic Ingredients: Importing Libraries\n",
    "# Import the necessary modules for web scraping and data storage.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import csv\n",
    "import os\n",
    "from queue import Queue\n",
    "\n",
    "# The Scroll of Constants: Define Global Variables\n",
    "# Constants that shall not change during the script's execution are defined here.\n",
    "BASE_URL = 'https://www.firstpeople.us/FP-Html-Legends/'\n",
    "BASE_FOLDER_PATH = '/Users/francinasimone/Desktop/DreamApp1/Starlight/Test/'\n",
    "\n",
    "# Elixir of Cleanliness: A String Cleaning Potion\n",
    "# Function to remove unwanted characters from a string, making it URL and filesystem friendly.\n",
    "def clean_string(s):\n",
    "    return s.replace(\" \", \"_\").replace(\"'\", \"\")\n",
    "\n",
    "# Summoning the Magic Mirror: Scanning URLs\n",
    "# Function to fetch all story URLs on a given webpage.\n",
    "def scan_url_for_links(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    return [\n",
    "        urljoin(url, link.get('href'))\n",
    "        for link in soup.find_all('a')\n",
    "        if link.get('href') and link.get('href').endswith('.html')\n",
    "    ]\n",
    "\n",
    "# The Lore Keeper: Fetching Page Data\n",
    "# Function to scrape the title, tribe, and content of a story from a URL.\n",
    "def fetch_page_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    title = soup.find('h1').text if soup.find('h1') else 'Unknown'\n",
    "    tribe = soup.find('h2').text if soup.find('h2') else 'Unknown'\n",
    "    content = soup.find('div', class_=\"content\").text if soup.find('div', class_=\"content\") else 'Content Missing'\n",
    "    \n",
    "    return title, tribe, content\n",
    "\n",
    "# Scribing the Tome: Saving to CSV\n",
    "# Function to save the scraped data to a CSV file.\n",
    "def save_to_csv(title, tribe, content):\n",
    "    folder_path = os.path.join(BASE_FOLDER_PATH, clean_string(tribe))\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    file_path = os.path.join(folder_path, f\"{clean_string(title)}.csv\")\n",
    "    \n",
    "    with open(file_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([title, tribe])\n",
    "        writer.writerow([content])\n",
    "\n",
    "# The Hero's Journey: Main Function\n",
    "# The main function where our scraping adventure begins.\n",
    "def main():\n",
    "    # Quest Log: Initialize Queue for BFS\n",
    "    link_queue = Queue()\n",
    "    \n",
    "    # The Gathering: Add Links to Queue\n",
    "    for link in scan_url_for_links(BASE_URL):\n",
    "        link_queue.put(link)\n",
    "        \n",
    "    # The Trials: Process Each Link in the Queue\n",
    "    while not link_queue.empty():\n",
    "        current_link = link_queue.get()\n",
    "        \n",
    "        print(f\"Scraping: {current_link}\")\n",
    "        title, tribe, content = fetch_page_data(current_link)\n",
    "        \n",
    "        print(f\"Title: {title}\\nTribe: {tribe}\\nContent Length: {len(content)}\")\n",
    "        \n",
    "        # The Reward: Save Data\n",
    "        save_to_csv(title, tribe, content)\n",
    "\n",
    "# The Portal: Where the Program Starts\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Scraper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Webscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "from queue import Queue\n",
    "\n",
    "class WebScraper:\n",
    "    # Constructor of the Alchemical Circle\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "\n",
    "    # The Astral Projector: Scans a URL for Usable Links\n",
    "    def scan_url_for_links(self):\n",
    "        response = requests.get(self.base_url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = []\n",
    "        \n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            if href and href.endswith('.html') and 'cookie' not in href.lower() and 'privacy' not in href.lower():\n",
    "                full_url = urljoin(self.base_url, href)\n",
    "                links.append(full_url)\n",
    "        return links\n",
    "\n",
    "    # The Lore Master: Extracts the Essence of a Single Page\n",
    "    def scrape_page_data(self, current_link):\n",
    "        page = requests.get(current_link)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        # Extracting the Sacred Title\n",
    "        story_title_tag = soup.find('h1')\n",
    "        story_title = story_title_tag.text if story_title_tag else \"Unknown Title\"\n",
    "\n",
    "        # Unveiling the Tribe of Origin\n",
    "        h2_tag = soup.find('h2')\n",
    "        text = h2_tag.text if h2_tag else \"Unknown Tribe\"\n",
    "        words = text.split()\n",
    "        words_to_strip = [\"An\", \"Legend\"]\n",
    "        tribe = \" \".join([word for word in words if word not in words_to_strip])\n",
    "\n",
    "        # Scribing the Lore\n",
    "        story_content_tag = soup.find('div', class_=\"content\")\n",
    "        story_text = story_content_tag.text if story_content_tag else \"Content Missing\"\n",
    "\n",
    "        self.save_csv_to_drive(story_text, tribe, story_title)\n",
    "\n",
    "        return story_title, tribe, story_text\n",
    "\n",
    "    # The Time Traveler: Scrapes Links with Breadth-First Search\n",
    "    def scrape_links_bfs(self, link_list):\n",
    "        queue = Queue()\n",
    "        \n",
    "        for link in link_list:\n",
    "            queue.put(link)\n",
    "            \n",
    "        while not queue.empty():\n",
    "            current_link = queue.get()\n",
    "            \n",
    "            print(f\"Processing link: {current_link}\")\n",
    "            story_title, tribe, story_text = self.scrape_page_data(current_link)\n",
    "            \n",
    "            print(story_title)\n",
    "            print(tribe)\n",
    "            print(story_text)\n",
    "\n",
    "    # The Scribe: Stores Extracted Lore as CSV Files\n",
    "    def save_csv_to_drive(self, story_text, tribe, story_title):\n",
    "        cleaned_tribe = re.sub(r'\\W+', '_', tribe)\n",
    "        cleaned_title = re.sub(r'\\W+', '_', story_title)\n",
    "        \n",
    "        folder_path = '/Users/francinasimone/Desktop/DreamApp/Starlight/First_People/' + cleaned_tribe + '/'\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        \n",
    "        file_path = folder_path + cleaned_title + '.csv'\n",
    "        \n",
    "        csv_data = [[story_text]]\n",
    "        \n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(csv_data)\n",
    "\n",
    "# Example usage\n",
    "scraper = WebScraper('https://www.firstpeople.us/FP-Html-Legends/')\n",
    "link_list = scraper.scan_url_for_links()\n",
    "scraper.scrape_links_bfs(link_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualdream",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
